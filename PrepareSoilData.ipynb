{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare soil data for use in LIM \n",
    "<br>\n",
    "\n",
    "<b>Primary Author:</b>  Meg D. Fowler, CIRES/NOAA PSD <br>\n",
    "<b>Date:</b>    6 Feb 2020 <br>\n",
    "<b>Name:</b>    Prepare soil data\n",
    "\n",
    "<br>\n",
    "<b>Short Description</b> <br>\n",
    "- Read in soil temperature and moisture observations from files containing NOAA HMT and US CRN data. <br>\n",
    "- Center the data by removing the mean <br>\n",
    "- Remove the average annual cycle from the data linearly <br>\n",
    "- Get z-scores of soil anomalies <br>\n",
    "- Linear regression between soil temperature and moisture to isolate only *residual* soil temperature <br>\n",
    "<br><br>\n",
    "Station order: [CZC, HBG, LSN, PTV, ROD, WLS, BDG, MRC, RED, STB, YSM]<br>\n",
    "<br>Note: This script is used for producing data in <i>Fowler and Penland</i> (2020; in prep).<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:55.219453Z",
     "start_time": "2020-02-07T17:50:55.214601Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from   numpy import linalg as LA\n",
    "import pandas as pd\n",
    "from   datetime import datetime\n",
    "from   dateutil import tz\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:29.156632Z",
     "start_time": "2020-02-07T17:50:29.149888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define some basics \n",
    "stationIDs = ['CZC','HBG','LSN','PTV','ROD','WLS','BDG','MRC','RED','STB','YSM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Read in soil data\n",
    "\n",
    "- Data is saved locally on NOAA machines, so paths will not work outside of this access. Note also that NOAA HMT data is not publicly available, but US-CRN data is. <br>\n",
    "- HMT data is saved in complex ways such that a single station may have multiple files for a single year, often reflecting added instrumentation. Thus a somewaht complex function is written to handle each case specifically. <br>\n",
    "- Times are converted from UTC to local California time for both soil networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.1 Define functions to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:34:19.544839Z",
     "start_time": "2020-02-06T22:34:19.533919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Define function to move from UTC to local California time \n",
    "\n",
    "def getLocTime(calenDay,year,time):\n",
    "    localDate  = np.empty(len(time), dtype='datetime64[s]')     #Empty array to save local time into \n",
    "    \n",
    "    # Settings for time conversion \n",
    "    from_zone = tz.gettz('UTC')                     #Time zone data is currently saved in\n",
    "    to_zone   = tz.gettz('America/Los_Angeles')     #Time zone you *want* data to be in instead \n",
    "    \n",
    "    #-- Convert UTC to Pacific Time --#\n",
    "    for iT in np.arange(0,len(time)):\n",
    "        strDay  = str(int(np.asarray(calenDay[iT]))).zfill(3)\n",
    "        strYr   = str(int(np.asarray(year[iT])))\n",
    "        strHr   = str(int(np.asarray(time[iT]))).zfill(4)[:2]\n",
    "        \n",
    "        utc     = datetime.strptime(strYr+' '+strDay+' '+strHr, '%Y %j %H')\n",
    "        # Tell the datetime object that it's in UTC time zone  \n",
    "        utc     = utc.replace(tzinfo=from_zone)\n",
    "        # Convert time zone\n",
    "        pacTime = utc.astimezone(to_zone)\n",
    "        \n",
    "        # Save local date for given time \n",
    "        localDate[iT] = pacTime.replace(tzinfo=None)\n",
    "    # ------- \n",
    "    \n",
    "    return localDate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:34:20.240201Z",
     "start_time": "2020-02-06T22:34:20.229736Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Define function to read in soil MOISTURE from the US-CRN network files\n",
    "\n",
    "def readData_CRN(fileName):\n",
    "    # Read in file data\n",
    "    smData         = pd.read_csv(fileName, sep=\",\", header=0,skiprows=1)\n",
    "    smData.columns = [\"network\",\"stationID\",\"Year\",\"Month\",\"Day\",\"Doy\",\"sm_5cm\",\"sm_10cm\",\n",
    "                      \"sm_20cm\",\"sm_50cm\",\"sm_100cm\",\"Lat\",\"Lon\",\"flag_depth1\",\"flag_depth2\",\n",
    "                      \"flag_depth3\",\"flag_depth4\",\"flag_depth5\"]\n",
    "    \n",
    "    # Get date as a single value\n",
    "    dt = np.empty(len(smData.Year), dtype='datetime64[s]')\n",
    "    for iT in range(len(smData.Year)):\n",
    "        dt[iT] = datetime(year=int(smData.Year[iT]), month=int(smData.Month[iT]), day=int(smData.Day[iT]))\n",
    "\n",
    "    # Add local time as column to pandas DF and drop year/month/day\n",
    "    smData.insert(0,\"Date\",dt,True)\n",
    "    smData = smData.drop('Year', axis=1)\n",
    "    smData = smData.drop('Month', axis=1)\n",
    "    smData = smData.drop('Day', axis=1)\n",
    "    \n",
    "    return smData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:34:20.896585Z",
     "start_time": "2020-02-06T22:34:20.885732Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define function to return soil TEMPERATURE from CRN network in addition to soil moisture, which is returned above\n",
    "\n",
    "def readAllCRN(stationSpec): \n",
    "    mainPath = '/home/mfowler/Python/Data/CRN_CAdata/OriginalFiles/CRND0103-'\n",
    "\n",
    "    yrRange  = np.arange(2011,2020)\n",
    "\n",
    "    appended_data = []\n",
    "    for iYr in range(len(yrRange)):\n",
    "        fileName = mainPath+str(yrRange[iYr])+'-CA_'+stationSpec+'.txt'\n",
    "        yrData   = pd.read_csv(fileName,sep='\\s+',header=None)\n",
    "        yrData.columns = [\"WBANNO\",\"Date\",\"CRX_VN\",\"Lon\",\"Lat\",\"T_Daily_Max\",\"T_Daily_Min\",\"T_Daily_Mean\",\n",
    "                         \"T_Daily_Avg\",\"P_Daily_Calc\",\"SolaRad_Daily\",\"Sfc_T_Daily_Type\",\"Sfc_T_Daily_Max\",\n",
    "                         \"Sfc_T_Daily_Min\",\"Sfc_T_Daily_Avg\",\"RH_Daily_Max\",\"RH_Daily_Min\",\"RH_Daily_Avg\",\n",
    "                         \"SM_5_Daily\",\"SM_10_Daily\",\"SM_20_Daily\",\"SM_50_Daily\",\"SM_100_Daily\",\n",
    "                         \"ST_5_Daily\", \"ST_10_Daily\",\"ST_20_Daily\",\"ST_50_Daily\",\"ST_100_Daily\"]\n",
    "        if iYr==0:\n",
    "            appended_data = yrData\n",
    "        else:\n",
    "            appended_data = pd.concat([appended_data, yrData],sort=False, ignore_index=True)\n",
    "            \n",
    "    #Put date in datetime form\n",
    "    appended_data.Date = pd.to_datetime(appended_data.Date,format = \"%Y%m%d\")\n",
    "\n",
    "    return appended_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:34:21.646943Z",
     "start_time": "2020-02-06T22:34:21.575541Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define function to return data for selected stations from the NOAA HMT files \n",
    "#    Note: These are a bit messier, but this seems like the cleanest way to organize it all. \n",
    "#    Station options: czc, hbg, lsn, mdt, ptv, pvc, pvw, rod, rve, rvn, rvw, str, wls\n",
    "#   \"splitYear\" indicates the year in which two records are present. Set to 0 to use only default section or if no split.\n",
    "\n",
    "def readData_Russian(station,stationYears,splitYear):\n",
    "\n",
    "    # Set paths \n",
    "    mainPath = '/psd2data/Soil_Moisture_qc1/'\n",
    "    hrPath   = '/hourly/'\n",
    "\n",
    "    #Initialize arrays to save all the data at this station\n",
    "    appended_data = []\n",
    "\n",
    "    #Loop over years in selection \n",
    "    for iYr in np.arange(0,len(stationYears)):\n",
    "        \n",
    "        # --- Define filenames (many stations have special cases) and column names ---\n",
    "        if station=='czc':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"] \n",
    "            if iYr==0:\n",
    "                yrString = '2012B'\n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "            else:\n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        \n",
    "        elif station=='hbg':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\n",
    "                    \"soilM_10\",\"soilM_15\"]\n",
    "            if stationYears[iYr]==splitYear:                    #2013 is split in two\n",
    "                yrString = '2013A'\n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "                \n",
    "                colNames2 = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\"soilT_50\",\n",
    "                    \"soilM_10\",\"soilM_15\",\"soilM_50\"]\n",
    "                yrString2 = '2013B'   \n",
    "                fileName2 = mainPath+station+hrPath+station+yrString2+'.dat'\n",
    "            else:\n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        \n",
    "        elif station=='lsn':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\n",
    "                    \"soilM_10\",\"soilM_15\"]\n",
    "            if stationYears[iYr]==2017:                    #2017 is split in two\n",
    "                yrString = '2017A'   \n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "                \n",
    "                colNames2 = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\"soilT_50\",\n",
    "                    \"soilM_10\",\"soilM_15\",\"soilM_50\"]\n",
    "                \n",
    "                yrString2 = '2017B'\n",
    "                fileName2 = mainPath+station+hrPath+station+yrString2+'.dat'\n",
    "            else:\n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        \n",
    "        elif station=='mdt':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_30\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_30\"]\n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "            \n",
    "        elif station=='ptv':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\n",
    "                    \"soilM_10\",\"soilM_15\",]\n",
    "            if stationYears[iYr]==2017:                    #2017 is split in two\n",
    "                yrString = '2017A'   \n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "                \n",
    "                colNames2 = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\"soilT_50\",\n",
    "                    \"soilM_10\",\"soilM_15\",\"soilM_50\"]\n",
    "                yrString2  = '2017B'\n",
    "                fileName2 = mainPath+station+hrPath+station+yrString2+'.dat'\n",
    "            else:\n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        \n",
    "        elif station=='pvc':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_30\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_30\"]\n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        \n",
    "        elif station=='pvw':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"]\n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "            \n",
    "        elif station=='rod':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\n",
    "                    \"soilM_10\",\"soilM_15\"] \n",
    "            if stationYears[iYr]==2017:                    #2017 is split in two\n",
    "                yrString = '2017A'   \n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "                \n",
    "                colNames2 = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\"soilT_50\",\n",
    "                    \"soilM_10\",\"soilM_15\",\"soilM_50\"] \n",
    "                yrString2 = '2017B'\n",
    "                fileName2 = mainPath+station+hrPath+station+yrString2+'.dat'\n",
    "            else:\n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "                \n",
    "        elif station=='rve':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"]\n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "            \n",
    "        elif station=='rvn':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"] \n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        elif station=='rvw':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"] \n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        elif station=='str':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"] \n",
    "            if stationYears[iYr]==2017: \n",
    "                yrString = '2017A'   \n",
    "                fileName = mainPath+station+hrPath+station+yrString+'.dat'\n",
    "                \n",
    "                colNames2 = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_5\",\"soilT_10\",\"soilT_15\",\"soilT_20\",\"soilT_50\",\"soilT_100\",\n",
    "                    \"soilM_5\",\"soilM_10\",\"soilM_15\",\"soilM_20\",\"soilM_50\",\"soilM_100\"] \n",
    "                yrString2 = '2017B'\n",
    "                fileName2 = mainPath+station+hrPath+station+yrString2+'.dat'\n",
    "            else: \n",
    "                fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'\n",
    "        elif station=='wls':\n",
    "            colNames = [\"year\",\"calenDay\",\"time\",\"sfcT\",\"RH\",\"P\",\n",
    "                    \"soilT_10\",\"soilT_15\",\n",
    "                    \"soilM_10\",\"soilM_15\",]\n",
    "            fileName = mainPath+station+hrPath+station+str(stationYears[iYr])+'.dat'     \n",
    "        # -------------------------        \n",
    "        \n",
    "        #If block to handle potential case where data record is split up \n",
    "        if (splitYear==0 or stationYears[iYr]<splitYear): #Don't worry about split\n",
    "\n",
    "            #Read in file as a pandas dataframe \n",
    "            data         = pd.read_csv(fileName, sep=\",\", header=None, usecols=(np.arange(1,len(colNames)+1)))\n",
    "            data.columns = colNames\n",
    "        \n",
    "             # Get local time instead of UTC\n",
    "            localDate = getLocTime(data.calenDay,data.year,data.time)\n",
    "\n",
    "            # Add local time as column to pandas DF\n",
    "            data.insert(0,\"LocalTime\",localDate,True)\n",
    "\n",
    "\n",
    "            # Append this year's data onto the end of the last year\n",
    "            if iYr==0:\n",
    "                appended_data = data\n",
    "            elif iYr>0:\n",
    "                appended_data = pd.concat([appended_data, data],sort=False, ignore_index=True)\n",
    "\n",
    "        elif stationYears[iYr]==splitYear:         #Special case for year of data split\n",
    "            # --- First half of year --- #\n",
    "            \n",
    "            #Read in file as a pandas dataframe \n",
    "            data         = pd.read_csv(fileName, sep=\",\", header=None, usecols=(np.arange(1,len(colNames)+1)))\n",
    "            data.columns = colNames\n",
    "            \n",
    "            # Get local time instead of UTC\n",
    "            localDate = getLocTime(data.calenDay,data.year,data.time)\n",
    "\n",
    "            # Add local time as column to pandas DF\n",
    "            data.insert(0,\"LocalTime\",localDate,True)\n",
    "\n",
    "            #appended_data.append(data)\n",
    "            appended_data = pd.concat([appended_data, data],sort=False, ignore_index=True)\n",
    "\n",
    "            # --- Second half of year --- #\n",
    "            \n",
    "            #Read in file as a pandas dataframe \n",
    "            data         = pd.read_csv(fileName2, sep=\",\", header=None, usecols=(np.arange(1,len(colNames2)+1)))\n",
    "            data.columns = colNames2\n",
    "            \n",
    "            # Get local time instead of UTC\n",
    "            localDate = getLocTime(data.calenDay,data.year,data.time)\n",
    "\n",
    "            # Add local time as column to pandas DF\n",
    "            data.insert(0,\"LocalTime\",localDate,True)\n",
    "\n",
    "            #appended_data.append(data)\n",
    "            appended_data = pd.concat([appended_data, data],sort=False, ignore_index=True)\n",
    "\n",
    "        elif (stationYears[iYr]>splitYear): \n",
    "            data         = pd.read_csv(fileName, sep=\",\", header=None, usecols=(np.arange(1,len(colNames2)+1)))\n",
    "            data.columns = colNames2\n",
    "            \n",
    "            # Get local time instead of UTC\n",
    "            localDate = getLocTime(data.calenDay,data.year,data.time)\n",
    "\n",
    "            # Add local time as column to pandas DF\n",
    "            data.insert(0,\"LocalTime\",localDate,True)\n",
    "\n",
    "            #appended_data.append(data)\n",
    "            appended_data = pd.concat([appended_data, data],sort=False, ignore_index=True)\n",
    "\n",
    "            \n",
    "    #Create one giant pandas dataframe for whole period \n",
    "\n",
    "    #Remove any missing values (typically stored as -1000 or -999, but if SM<0, you know it's wrong [Bob's method])\n",
    "    appended_data.loc[(appended_data.soilM_15<0), :] = np.nan\n",
    "    appended_data.loc[(appended_data.soilM_15>2), :] = np.nan  #Really weird outlier data point\n",
    "    #Add second flag dependent on temperature - some weird stuff's when I plot it... \n",
    "    appended_data.loc[(appended_data.sfcT<-500), :] = np.nan\n",
    "    #Add another flag for precipitation -_- \n",
    "    appended_data.loc[(appended_data.P<0), :]    = np.nan\n",
    "    appended_data.loc[(appended_data.P>4000), :] = np.nan  #Remove bad data point in HBG\n",
    "    \n",
    "    return appended_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2 Read in the actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in NOAA HMT data, which is saved in individual (1-2) yearly files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:35:29.592439Z",
     "start_time": "2020-02-06T22:34:23.028529Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with CZC\n",
      "Done with HBG\n",
      "Done with LSN\n",
      "Done with PTV\n",
      "Done with ROD\n",
      "Done with WLS\n"
     ]
    }
   ],
   "source": [
    "#Station: CZC\n",
    "appended_data_czc = readData_Russian('czc',np.arange(2012,2018),0)\n",
    "dailyAvg_czc      = appended_data_czc.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with CZC')\n",
    "\n",
    "#Station: HBG\n",
    "appended_data_hbg = readData_Russian('hbg',np.arange(2006,2018),2013)\n",
    "dailyAvg_hbg      = appended_data_hbg.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with HBG')\n",
    "\n",
    "#Station: LSN\n",
    "appended_data_lsn = readData_Russian('lsn',np.arange(2010,2018),2017)\n",
    "dailyAvg_lsn      = appended_data_lsn.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with LSN')\n",
    "\n",
    "#Station: PTV\n",
    "appended_data_ptv = readData_Russian('ptv',np.arange(2011,2018),2017)\n",
    "dailyAvg_ptv      = appended_data_ptv.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with PTV')\n",
    "\n",
    "#Station: ROD\n",
    "appended_data_rod = readData_Russian('rod',np.arange(2006,2018),2017)\n",
    "dailyAvg_rod      = appended_data_rod.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with ROD')\n",
    "\n",
    "#Station: WLS\n",
    "appended_data_wls = readData_Russian('wls',np.arange(2010,2018),0)\n",
    "dailyAvg_wls      = appended_data_wls.resample('D', on = 'LocalTime').mean()   #Get daily average \n",
    "print('Done with WLS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in US CRN data. Soil moisture is read in first, as it retrieved from the National Soil Moisture Network. To look at soil temperature as well, we read in the raw CRN files second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Define file paths\n",
    "mainPath      = '/home/mfowler/Python/Data/CRN_CAdata/'\n",
    "\n",
    "BodegaFile    = mainPath+'CRNH0203-2018-CA_Bodega_6_WSW.csv'\n",
    "MercedFile    = mainPath+'CRNH0203-2018-CA_Merced_23_WSW.csv'\n",
    "ReddingFile   = mainPath+'CRNH0203-2018-CA_Redding_12_WNW.csv'\n",
    "SBfile        = mainPath+'CRNH0203-2018-CA_Santa_Barbara_11_W.csv'\n",
    "YosemiteFile  = mainPath+'CRNH0203-2018-CA_Yosemite_Village_12_W.csv'\n",
    "\n",
    "#Read in data for each station \n",
    "sm_Bodega    = readData_CRN(BodegaFile)\n",
    "sm_Merced    = readData_CRN(MercedFile)\n",
    "sm_Redding   = readData_CRN(ReddingFile)\n",
    "sm_SantaBarb = readData_CRN(SBfile)\n",
    "sm_Yosemite  = readData_CRN(YosemiteFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read in the full CRN files for each station (to grab temperature)\n",
    "BDG_allCRN = readAllCRN('Bodega_6_WSW')\n",
    "MRC_allCRN = readAllCRN('Merced_23_WSW')\n",
    "RED_allCRN = readAllCRN('Redding_12_WNW')\n",
    "STB_allCRN = readAllCRN('Santa_Barbara_11_W')\n",
    "YSM_allCRN = readAllCRN('Yosemite_Village_12_W')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.3 Restrict data to common period between all stations/networks\n",
    "- CRN data extends consistently 2011-2019, but HMT stations vary in length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Station      Start Date               End Date   ')\n",
    "print('-----------------------------------------------------')\n",
    "print('CZC      ', dailyAvg_czc.index[0],'   ', dailyAvg_czc.index[-1])\n",
    "print('HBG      ', dailyAvg_hbg.index[0],'   ', dailyAvg_hbg.index[-1])   \n",
    "print('LSN      ', dailyAvg_lsn.index[0],'   ', dailyAvg_lsn.index[-1])\n",
    "print('PTV      ', dailyAvg_ptv.index[0],'   ', dailyAvg_ptv.index[-1])\n",
    "print('ROD      ', dailyAvg_rod.index[0],'   ', dailyAvg_rod.index[-1])\n",
    "print('WLS      ', dailyAvg_wls.index[0],'   ', dailyAvg_wls.index[-1])\n",
    "print('BDG      ', sm_Bodega[\"Date\"].iloc[0],'   ', sm_Bodega[\"Date\"].iloc[-1])\n",
    "print('MRC      ', sm_Merced[\"Date\"].iloc[0],'   ', sm_Merced[\"Date\"].iloc[-1])\n",
    "print('RED      ', sm_Redding[\"Date\"].iloc[0],'   ', sm_Redding[\"Date\"].iloc[-1])\n",
    "print('STB      ', sm_SantaBarb[\"Date\"].iloc[0],'   ', sm_SantaBarb[\"Date\"].iloc[-1])\n",
    "print('YSM      ', sm_Yosemite[\"Date\"].iloc[0],'   ', sm_Yosemite[\"Date\"].iloc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "startDate = '2012-05-23'\n",
    "endDate   = '2017-12-31'\n",
    "\n",
    "# Select time period in NOAA HMT data \n",
    "CZC_sel = dailyAvg_czc.loc[(dailyAvg_czc.index >= startDate)]\n",
    "HBG_sel = dailyAvg_hbg.loc[(dailyAvg_hbg.index >= startDate)]\n",
    "LSN_sel = dailyAvg_lsn.loc[(dailyAvg_lsn.index >= startDate)]\n",
    "PTV_sel = dailyAvg_ptv.loc[(dailyAvg_ptv.index >= startDate)]\n",
    "ROD_sel = dailyAvg_rod.loc[(dailyAvg_rod.index >= startDate)]\n",
    "WLS_sel = dailyAvg_wls.loc[(dailyAvg_wls.index >= startDate)]\n",
    "\n",
    "# Select time period in soil moisture CRN data \n",
    "BDG_sel = sm_Bodega.loc[((sm_Bodega.Date       >= startDate) & (sm_Bodega.Date <= endDate))]\n",
    "MRC_sel = sm_Merced.loc[((sm_Merced.Date       >= startDate) & (sm_Merced.Date <= endDate))] \n",
    "RED_sel = sm_Redding.loc[((sm_Redding.Date     >= startDate) & (sm_Redding.Date <= endDate))]\n",
    "STB_sel = sm_SantaBarb.loc[((sm_SantaBarb.Date >= startDate) & (sm_SantaBarb.Date <= endDate))]\n",
    "YSM_sel = sm_Yosemite.loc[((sm_Yosemite.Date   >= startDate) & (sm_Yosemite.Date <= endDate))]\n",
    "\n",
    "# Select the same time period for everything in soil temperature arrays as well \n",
    "BDG_AllSel = BDG_allCRN.loc[((BDG_allCRN.Date   >= startDate) & (BDG_allCRN.Date <= endDate))]\n",
    "MRC_AllSel = MRC_allCRN.loc[((MRC_allCRN.Date   >= startDate) & (MRC_allCRN.Date <= endDate))] \n",
    "RED_AllSel = RED_allCRN.loc[((RED_allCRN.Date   >= startDate) & (RED_allCRN.Date <= endDate))]\n",
    "STB_AllSel = STB_allCRN.loc[((STB_allCRN.Date   >= startDate) & (STB_allCRN.Date <= endDate))]\n",
    "YSM_AllSel = YSM_allCRN.loc[((YSM_allCRN.Date   >= startDate) & (YSM_allCRN.Date <= endDate))]\n",
    "\n",
    "#Handle missing data \n",
    "BDG_AllSel.loc[(BDG_AllSel.ST_10_Daily<=-9999.0), :] = np.nan\n",
    "MRC_AllSel.loc[(MRC_AllSel.ST_10_Daily<=-9999.0), :] = np.nan\n",
    "RED_AllSel.loc[(RED_AllSel.ST_10_Daily<=-9999.0), :] = np.nan\n",
    "STB_AllSel.loc[(STB_AllSel.ST_10_Daily<=-9999.0), :] = np.nan\n",
    "YSM_AllSel.loc[(YSM_AllSel.ST_10_Daily<=-9999.0), :] = np.nan\n",
    "\n",
    "#Define date array\n",
    "dates      = BDG_sel.Date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2. Center the data\n",
    "Remove the mean from each station individually in terms of soil moisture and temperature (at 10cm here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Center each timeseries of soil MOISTURE (i.e., remove the mean)\n",
    "\n",
    "CZC_cent = CZC_sel.soilM_10 - np.nanmean(CZC_sel.soilM_10)\n",
    "HBG_cent = HBG_sel.soilM_10 - np.nanmean(HBG_sel.soilM_10)\n",
    "LSN_cent = LSN_sel.soilM_10 - np.nanmean(LSN_sel.soilM_10)\n",
    "PTV_cent = PTV_sel.soilM_10 - np.nanmean(PTV_sel.soilM_10)\n",
    "ROD_cent = ROD_sel.soilM_10 - np.nanmean(ROD_sel.soilM_10)\n",
    "WLS_cent = WLS_sel.soilM_10 - np.nanmean(WLS_sel.soilM_10)\n",
    "\n",
    "BDG_cent = BDG_sel.sm_10cm - np.nanmean(BDG_sel.sm_10cm)\n",
    "MRC_cent = MRC_sel.sm_10cm - np.nanmean(MRC_sel.sm_10cm)\n",
    "RED_cent = RED_sel.sm_10cm - np.nanmean(RED_sel.sm_10cm)\n",
    "STB_cent = STB_sel.sm_10cm - np.nanmean(STB_sel.sm_10cm)\n",
    "YSM_cent = YSM_sel.sm_10cm - np.nanmean(YSM_sel.sm_10cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Center each timeseries of soil TEMPERATURE (i.e., remove the mean)\n",
    "\n",
    "CZC_centST = CZC_sel.soilT_10 - np.nanmean(CZC_sel.soilT_10)\n",
    "HBG_centST = HBG_sel.soilT_10 - np.nanmean(HBG_sel.soilT_10)\n",
    "LSN_centST = LSN_sel.soilT_10 - np.nanmean(LSN_sel.soilT_10)\n",
    "PTV_centST = PTV_sel.soilT_10 - np.nanmean(PTV_sel.soilT_10)\n",
    "ROD_centST = ROD_sel.soilT_10 - np.nanmean(ROD_sel.soilT_10)\n",
    "WLS_centST = WLS_sel.soilT_10 - np.nanmean(WLS_sel.soilT_10)\n",
    "\n",
    "BDG_centST = BDG_AllSel.ST_10_Daily - np.nanmean(BDG_AllSel.ST_10_Daily)\n",
    "MRC_centST = MRC_AllSel.ST_10_Daily - np.nanmean(MRC_AllSel.ST_10_Daily)\n",
    "RED_centST = RED_AllSel.ST_10_Daily - np.nanmean(RED_AllSel.ST_10_Daily)\n",
    "STB_centST = STB_AllSel.ST_10_Daily - np.nanmean(STB_AllSel.ST_10_Daily)\n",
    "YSM_centST = YSM_AllSel.ST_10_Daily - np.nanmean(YSM_AllSel.ST_10_Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-06T22:55:58.138936Z",
     "start_time": "2020-02-06T22:55:57.468287Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b2cad10857fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstationIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'CZC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HBG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'LSN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PTV'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ROD'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WLS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'BDG'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MRC'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'RED'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'STB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'YSM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m smArray = np.asarray([CZC_cent, HBG_cent, LSN_cent, PTV_cent, ROD_cent, WLS_cent,\n\u001b[0m\u001b[1;32m      5\u001b[0m                       BDG_cent, MRC_cent, RED_cent, STB_cent, YSM_cent])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Create large array for soil moisture\n",
    "smArray = np.asarray([CZC_cent, HBG_cent, LSN_cent, PTV_cent, ROD_cent, WLS_cent,\n",
    "                      BDG_cent, MRC_cent, RED_cent, STB_cent, YSM_cent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create large array for soil temperature\n",
    "stArray = np.asarray([CZC_centST, HBG_centST, LSN_centST, PTV_centST, ROD_centST, WLS_centST,\n",
    "                      BDG_centST, MRC_centST, RED_centST, STB_centST, YSM_centST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 3. Compute average annual cycle \n",
    "- Leap year in 2016 is ignored here (assuming a constant 365-day year) \n",
    "- Soil data is first split into years, then each calendar day is averaged over the 5-6 year period available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Find the index of the first day of each year \n",
    "yrFind   = np.arange(2013,2018)\n",
    "yrStarts = np.empty([5])\n",
    "\n",
    "for iStart in range(len(yrFind)):\n",
    "    yrStarts[iStart] = np.where(CZC_sel.index == str(yrFind[iStart])+'-01-01')[0]\n",
    "\n",
    "#Ignore leap year day Feb 29 in 2016\n",
    "dateIgnore = np.where(CZC_sel.index == '2016-02-29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Re-define each station's record to be [yr x day] rather than [days]\n",
    "# May 23rd is the 144th day in a year \n",
    "\n",
    "#Define initially empty arrays to store data in \n",
    "yearlySM = np.full([11,6,365],np.nan) #Dimensions of [station, yr, day of year]\n",
    "yearlyST = np.full([11,6,365],np.nan) #Dimensions of [station, yr, day of year]\n",
    "\n",
    "for iYr in range(6):\n",
    "    #Handle the first year (doesn't start in Jan)\n",
    "    if iYr==0:\n",
    "        yearlySM[:,0,142::] = smArray[:, 0:int(yrStarts[0])]\n",
    "        yearlyST[:,0,142::] = stArray[:, 0:int(yrStarts[0])]\n",
    "    #Handle leap year - it's annoying\n",
    "    elif yrFind[iYr-1]==2016: \n",
    "        #Feb 29th is the 60th day of the year, so index=59\n",
    "        yearlySM[:,iYr,0:59] = smArray[:, int(yrStarts[iYr-1]):int(dateIgnore[0])]\n",
    "        yearlySM[:,iYr,59::] = smArray[:, int(dateIgnore[0])+1:int(yrStarts[iYr])]\n",
    "        yearlyST[:,iYr,0:59] = stArray[:, int(yrStarts[iYr-1]):int(dateIgnore[0])]\n",
    "        yearlyST[:,iYr,59::] = stArray[:, int(dateIgnore[0])+1:int(yrStarts[iYr])]\n",
    "    #Handle the last year case \n",
    "    elif iYr==5:   #If last year of data\n",
    "        yearlySM[:,iYr,:]    = smArray[:,int(yrStarts[iYr-1])::]\n",
    "        yearlyST[:,iYr,:]    = stArray[:,int(yrStarts[iYr-1])::]\n",
    "    else: \n",
    "        yearlySM[:,iYr,:]    = smArray[:,int(yrStarts[iYr-1]):int(yrStarts[iYr])]\n",
    "        yearlyST[:,iYr,:]    = stArray[:,int(yrStarts[iYr-1]):int(yrStarts[iYr])]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Get the *average* annual cycle by averaging over the year axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get annual average cycle \n",
    "annCycle_SM = np.nanmean(yearlySM,axis=1)\n",
    "annCycle_ST = np.nanmean(yearlyST,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4. Remove the annual cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove annual cycle from soil MOISTURE data \n",
    "\n",
    "annCycle_SM_allDays = np.full([11,len(CZC_sel.index)],np.nan)\n",
    "\n",
    "for iYr in range(len(yrStarts)+1): \n",
    "    #Handle case of first year(starts in May)\n",
    "    if iYr==0:\n",
    "        annCycle_SM_allDays[:,0:int(yrStarts[0])]   = smArray[:,0:int(yrStarts[0])]-annCycle_SM[:,142::]\n",
    "    \n",
    "    #Handle case of the last year \n",
    "    elif iYr==len(yrStarts):\n",
    "        annCycle_SM_allDays[:,int(yrStarts[iYr-1])::] = smArray[:,int(yrStarts[iYr-1])::]-annCycle_SM[:,:]\n",
    "    \n",
    "    #Handle leap year case\n",
    "    elif iYr==4:\n",
    "        annCycle_SM_allDays[:,int(yrStarts[iYr-1]):int(yrStarts[iYr-1]+59)] = smArray[:,int(yrStarts[iYr-1]):int(dateIgnore[0])]-annCycle_SM[:,0:59]\n",
    "        annCycle_SM_allDays[:,int(yrStarts[iYr-1]+60):int((yrStarts[iYr]))] = smArray[:,int(dateIgnore[0]+1):int(yrStarts[iYr])]-annCycle_SM[:,59::]\n",
    "    \n",
    "    #Handle other years \n",
    "    else: \n",
    "        annCycle_SM_allDays[:,int(yrStarts[iYr-1]):int((yrStarts[iYr]))]    = smArray[:,int(yrStarts[iYr-1]):int(yrStarts[iYr])]-annCycle_SM[:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove annual cycle from soil TEMPERATURE data \n",
    "\n",
    "annCycle_ST_allDays = np.full([11,len(CZC_sel.index)],np.nan)\n",
    "\n",
    "for iYr in range(len(yrStarts)+1): \n",
    "    #Handle case of first year(starts in May)\n",
    "    if iYr==0:\n",
    "        annCycle_ST_allDays[:,0:int(yrStarts[0])]   = stArray[:,0:int(yrStarts[0])]-annCycle_ST[:,142::]\n",
    "        \n",
    "    #Handle case of the last year \n",
    "    elif iYr==len(yrStarts):\n",
    "        annCycle_ST_allDays[:,int(yrStarts[iYr-1])::] = stArray[:,int(yrStarts[iYr-1])::]-annCycle_ST[:,:]\n",
    "    \n",
    "    #Handle leap year case\n",
    "    elif iYr==4:\n",
    "        annCycle_ST_allDays[:,int(yrStarts[iYr-1]):int(yrStarts[iYr-1]+59)] = stArray[:,int(yrStarts[iYr-1]):int(dateIgnore[0])]-annCycle_ST[:,0:59]\n",
    "        annCycle_ST_allDays[:,int(yrStarts[iYr-1]+60):int((yrStarts[iYr]))] = stArray[:,int(dateIgnore[0]+1):int(yrStarts[iYr])]-annCycle_ST[:,59::]\n",
    "    \n",
    "    #Handle other years \n",
    "    else: \n",
    "        annCycle_ST_allDays[:,int(yrStarts[iYr-1]):int((yrStarts[iYr]))]    = stArray[:,int(yrStarts[iYr-1]):int(yrStarts[iYr])]-annCycle_ST[:,:]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 5. Write out data (centered with annual cycle removed) to text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # -- Write file with soil MOISTURE info \n",
    "\n",
    "#Write out the Eigenvalues and vectors to a file too \n",
    "fileName = '/home/mfowler/Python/textFiles/CombinedNetworks/SM_withoutAvgAnnCycle.txt'\n",
    "\n",
    "#Replacing missing values with -999.0\n",
    "annCycle_SM_allDays[~np.isfinite(annCycle_SM_allDays)] = -999.0\n",
    "\n",
    "with open(fileName,'w+') as f: \n",
    "    for iT in range(len(dates)): \n",
    "        f.write(\"%s %f %f %f %f %f %f %f %f %f %f %f \\n\" % (np.asarray(dates)[iT],     annCycle_SM_allDays[0,iT],\n",
    "                                                            annCycle_SM_allDays[1,iT], annCycle_SM_allDays[2,iT],\n",
    "                                                            annCycle_SM_allDays[3,iT], annCycle_SM_allDays[4,iT], \n",
    "                                                            annCycle_SM_allDays[5,iT], annCycle_SM_allDays[6,iT],\n",
    "                                                            annCycle_SM_allDays[7,iT], annCycle_SM_allDays[8,iT],\n",
    "                                                            annCycle_SM_allDays[9,iT], annCycle_SM_allDays[10,iT]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # -- Write file with soil TEMPERATURE information \n",
    "\n",
    "#Write out the Eigenvalues and vectors to a file too \n",
    "fileName = '/home/mfowler/Python/textFiles/CombinedNetworks/ST_withoutAvgAnnCycle.txt'\n",
    "\n",
    "#Replacing missing values with -999.0\n",
    "annCycle_ST_allDays[~np.isfinite(annCycle_ST_allDays)] = -999.0\n",
    "\n",
    "with open(fileName,'w+') as f: \n",
    "    for iT in range(len(dates)): \n",
    "        f.write(\"%s %f %f %f %f %f %f %f %f %f %f %f \\n\" % (np.asarray(dates)[iT],     annCycle_ST_allDays[0,iT],\n",
    "                                                            annCycle_ST_allDays[1,iT], annCycle_ST_allDays[2,iT],\n",
    "                                                            annCycle_ST_allDays[3,iT], annCycle_ST_allDays[4,iT], \n",
    "                                                            annCycle_ST_allDays[5,iT], annCycle_ST_allDays[6,iT],\n",
    "                                                            annCycle_ST_allDays[7,iT], annCycle_ST_allDays[8,iT],\n",
    "                                                            annCycle_ST_allDays[9,iT], annCycle_ST_allDays[10,iT]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6. Get z-scores\n",
    "- This section on is structured to first read in the above text files, then keep on computing based on that. (Typically quicker than needing to run the whole thing again). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:31.887920Z",
     "start_time": "2020-02-07T17:50:31.837335Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Read in the data that has the average annual cycle removed, skipping the date column\n",
    "fileName_SM    = '/home/mfowler/Python/textFiles/CombinedNetworks/SM_withoutAvgAnnCycle.txt'\n",
    "SMdata         = pd.read_csv(fileName_SM, sep=\"\\s+\", header=None,usecols=np.arange(11)+1)\n",
    "SMdata.columns = stationIDs\n",
    "\n",
    "fileName_ST    = '/home/mfowler/Python/textFiles/CombinedNetworks/ST_withoutAvgAnnCycle.txt'\n",
    "STdata         = pd.read_csv(fileName_ST, sep=\"\\s+\", header=None,usecols=np.arange(11)+1) \n",
    "STdata.columns = stationIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:32.479644Z",
     "start_time": "2020-02-07T17:50:32.468338Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:       (22, 2049)\n",
      "Shape of transpose:  (2049, 22)\n"
     ]
    }
   ],
   "source": [
    "#Combine the two variables together (time x station/var)\n",
    "comboArr          = np.full([np.shape(np.asarray(SMdata))[0],np.shape(np.asarray(SMdata))[1]*2],np.nan)\n",
    "comboArr[:,0:11]  = STdata\n",
    "comboArr[:,11:22] = SMdata\n",
    "\n",
    "#Put dimensions in right order (station/var x time)\n",
    "allArray = np.transpose(np.asarray(comboArr))\n",
    "\n",
    "# Also get the transpose of the combined matrix \n",
    "allTranspose = np.transpose(allArray)\n",
    "\n",
    "print('Shape of data:      ', np.shape(allArray))\n",
    "print('Shape of transpose: ', np.shape(allTranspose))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:33.096240Z",
     "start_time": "2020-02-07T17:50:33.061210Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mfowler/.conda/envs/python3/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less_equal\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# -- Get the z-scores for each station/variable -- #\n",
    "nTimes = len(SMdata.CZC)\n",
    "\n",
    "#Properly set missing values to NaN from -999 \n",
    "allArray[allArray<=-999]         = np.nan\n",
    "allTranspose[allTranspose<=-999] = np.nan\n",
    "\n",
    "#Define empty arrays\n",
    "normAll   = np.full([len(stationIDs)*2, nTimes], np.nan)\n",
    "normAll_T = np.full([nTimes, len(stationIDs)*2], np.nan)\n",
    "\n",
    "#Remove the mean from each station record\n",
    "for i in range(len(stationIDs)*2):\n",
    "    normAll[i,:]  = (allArray[i,:]     - np.nanmean(allArray[i,:]))/np.nanstd(allArray[i,:])\n",
    "    normAll_T[:,i]= (allTranspose[:,i] - np.nanmean(allTranspose[:,i]))/np.nanstd(allTranspose[:,i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 7. Linear Regression\n",
    "- Based on contemporaneous covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:34.880895Z",
     "start_time": "2020-02-07T17:50:34.820261Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get contemporaneous covariance matrix\n",
    "nDat = len(normAll)\n",
    "\n",
    "c0   = np.full([nDat,nDat],np.nan)\n",
    "for iR in range(nDat):\n",
    "    for iC in range(nDat): \n",
    "        c0[iR,iC] = np.nansum(normAll[iR,:]*normAll_T[:,iC])/np.nansum(np.isfinite(normAll[iR,:]*normAll_T[:,iC]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:35.432464Z",
     "start_time": "2020-02-07T17:50:35.396223Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now we can get the regression coefficients from that matrix, c0 \n",
    "\n",
    "MM = c0[11:22,11:22]  #Bottom right quadrant\n",
    "TM = c0[0:11,11:22]   #Top right quadrant \n",
    "\n",
    "MM_inv = LA.inv(MM)   #Get the inverse of the moisture covariance matrix  \n",
    "\n",
    "regr   = np.dot(TM,MM_inv)  #Regression coefficients calcualted as product of these two matrices \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:48.382327Z",
     "start_time": "2020-02-07T17:50:36.052175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sum manually in order to do matrix multiplication and ignore missing values \n",
    "\n",
    "# Isolate ST and SM z-scores \n",
    "ST = normAll[0:11,:]\n",
    "SM = normAll[11:22,:]\n",
    "\n",
    "# Create empty arrays to hold T and M as estimated via regression \n",
    "ST_hat = np.zeros([len(regr),len(ST[0,:])])\n",
    "SM_hat = np.zeros([len(regr),len(SM[0,:])])\n",
    "\n",
    "# iterate through rows of X\n",
    "for i in range(len(regr)):\n",
    "   # iterate through columns of Y\n",
    "   for j in range(len(ST[0,:])):\n",
    "       # iterate through rows of Y\n",
    "       for k in range(len(ST)):\n",
    "            ST_hat[i,j] += np.nansum(regr[i,k] * SM[k,j])\n",
    "            SM_hat[i,j] += np.nansum(regr[i,k] * ST[k,j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:50:59.528882Z",
     "start_time": "2020-02-07T17:50:59.525057Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Compute the residual of temperature\n",
    "#   This removes the part of temperature that is already explained by the moisture field \n",
    "resid = ST - ST_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:51:00.374355Z",
     "start_time": "2020-02-07T17:51:00.357075Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 2049)\n"
     ]
    }
   ],
   "source": [
    "# -- Remove the mean again (center the data) -- #\n",
    "resid_T = np.transpose(resid)\n",
    "print(np.shape(resid))\n",
    "\n",
    "#Define empty arrays\n",
    "normResid   = np.full([len(stationIDs), nTimes], np.nan)\n",
    "normResid_T = np.full([nTimes, len(stationIDs)], np.nan)\n",
    "\n",
    "#Remove the mean from each station record\n",
    "for i in range(len(stationIDs)):\n",
    "    normResid[i,:]   = resid[i,:]   - np.nanmean(resid[i,:])\n",
    "    normResid_T[:,i] = resid_T[:,i] - np.nanmean(resid_T[:,i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:51:02.192899Z",
     "start_time": "2020-02-07T17:51:02.188471Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Replace the normalized S.T. with the normalized residual of S.T. in the full combined array \n",
    "normAll[0:11,:] = normResid\n",
    "normAll_T       = np.transpose(normAll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save new array of daily, anomalous, residual soil temperature and soil moisture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-07T17:52:46.216179Z",
     "start_time": "2020-02-07T17:52:46.172020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the array with residaul S.T. and S.M. z-scores in it \n",
    "pickle.dump( normAll, open( \"/home/mfowler/Python/pickleFiles/zScores_residST_SM.p\", \"wb\" ) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
